{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":28785,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":8318,"modelId":3301},{"sourceId":28808,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":8332,"modelId":3301}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers accelerate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clone your GitHub repo into Kaggle\n!git clone https://github.com/abhikad292/abhinav_sample.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd abhinav_sample/llm-prompt-comparison","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # suppress TensorFlow/XLA warnings\n# Importing the transformers library classes\n# AutoTokenizer -> used to convert human text into tokens (numbers) the model can understand\n# AutoModelForCausalLM -> loads a pre-trained language model capable of generating text (Causal LM = causal language model)\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n\n# Step 1: Choose the model you want to use\n# Here we are using \"Gemma 7B IT\", an open-source LLM from Google\nmodel_name = \"/kaggle/input/gemma/transformers/2b-it/3\"\n\n# Step 2: Load the tokenizer\n# A tokenizer converts text (your input prompt) into numbers (tokens) that the model can understand\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Step 3: Load the model into memory\n# AutoModelForCausalLM loads a causal language model (good for text generation tasks)\nmodel = AutoModelForCausalLM.from_pretrained(model_name,device_map=\"auto\", offload_folder=\"offload\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 4: Define query function (fixed temperature issue)\ndef query_model(prompt):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    \n    # Define generation configuration\n    gen_config = GenerationConfig(\n        max_new_tokens=2048,\n        temperature=0.7,  # adjust randomness\n        do_sample=True     # required if using temperature\n    )\n    \n    outputs = model.generate(**inputs, generation_config=gen_config)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch, gc\ngc.collect()\ntorch.cuda.empty_cache()\n# Step 5: Load universal prompt from GitHub\nwith open(\"prompts/universal_prompt.txt\") as f:\n    universal_prompt = f.read()\n\n# Step 6: Add your test rules\nrule1 = \"\"\"Given the condition: If FICO > 750 and FICO <= 900 and NO_INQ >= 2 and NO_INQ <= 99 then DECISION_CD is 'Declined' and DECISION_DESC is 'INQUIRIES'.\nGenerate all combinations of min, mid, max for FICO and NO_INQ within these bounds.\nOutput in format: (FICO, NO_INQ, DECISION_CD, DECISION_DESC)\"\"\"\n\n# Step 7: Run the model\noutput1 = query_model(rule1)\nprint(\"Input 1 Output:\\n\", output1)\nwith open(\"open_source_input1.json\", \"w\") as f:\n    f.write(output1)\n\nrule2=\"\"\"Given the condition: If Income > 50000 and Debt_To_Income < 0.4 then Loan_status is Approved and Comment is 'Low Risk'.\nGenerate all the possible combination where loan status is approved and comment is low risk.\nOutput in format: (Income, Debt_To_Income, Loan_status, comment)\"\"\"\n\n# Step 7: Run the model\noutput2 = query_model(rule2)\nprint(\"Input 2 Output:\\n\", output2)\nwith open(\"open_source_input2.json\", \"w\") as f:\n    f.write(output2)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}