{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nComprehensive Installation Script for ML/NLP Pipeline\n==================================================\nThis script sets up a complete machine learning environment with document processing,\nvector embeddings, and conversational AI capabilities. Designed for Kaggle environments\nbut adaptable to other platforms.\n\nDependencies installed:\n- Document processing: PyMuPDF, Pillow, Tesseract OCR\n- ML/NLP: Transformers, Sentence Transformers, FAISS, Torch\n- Frameworks: LangChain ecosystem, Gradio for UI\n- Utilities: Accelerate, FSSpec for file handling\n\"\"\"\n\n# Import required system modules\nimport os  # For environment variable management\nimport sys  # For system-specific parameters and functions\n\n# =============================================================================\n# ENVIRONMENT CONFIGURATION\n# =============================================================================\n# Configure cache directories to avoid storage conflicts and improve performance\n# These paths are optimized for Kaggle's file system structure\n\n# Set Transformers library cache location to working directory\n# Prevents downloading models to default cache which may have space limitations\nos.environ[\"TRANSFORMERS_CACHE\"] = \"/kaggle/working/transformers_cache\"\n\n# Set Hugging Face Hub cache location for model storage\n# Centralizes all HF model downloads to a single, manageable location\nos.environ[\"HF_HOME\"] = \"/kaggle/working/huggingface_cache\"\n\n# =============================================================================\n# DOCUMENT PROCESSING LIBRARIES\n# =============================================================================\n# Install PDF processing and OCR capabilities\n\n# PyMuPDF v1.23.0 - High-performance PDF processing library\n# Pinned version for stability and compatibility\n!pip install --upgrade --quiet PyMuPDF==1.23.0\n\n# Install image processing and OCR suite\n# PyMuPDF: PDF manipulation and text extraction\n# Pillow: Python Imaging Library for image processing\n# pytesseract: Python wrapper for Tesseract OCR engine\n!pip install --upgrade --quiet \\\n    PyMuPDF pillow pytesseract\n\n# =============================================================================\n# MACHINE LEARNING AND NLP CORE LIBRARIES\n# =============================================================================\n# Install foundational ML libraries for embedding and similarity search\n\n# Sentence Transformers - Pre-trained models for semantic text embeddings\n# Enables conversion of text to high-dimensional vectors for similarity analysis\n!pip install --upgrade --quiet sentence-transformers\n\n# FAISS (Facebook AI Similarity Search) - Efficient similarity search library\n# CPU version for vector indexing and nearest neighbor search\n!pip install --upgrade --quiet faiss-cpu\n\n# Core ML framework installations\n# Transformers: Hugging Face's transformer models library\n# Torch: PyTorch deep learning framework\n!pip install --upgrade --quiet transformers torch\n\n# =============================================================================\n# USER INTERFACE AND ACCELERATION\n# =============================================================================\n# Install libraries for model acceleration and web interface creation\n\n# Gradio - Python library for creating ML web interfaces\n# Enables easy deployment of models with interactive web UIs\n!pip install --upgrade --quiet gradio\n\n# Accelerate - Hugging Face library for distributed training and inference\n# Optimizes model loading and computation across different hardware configurations\n!pip install --upgrade --quiet accelerate\n\n# Redundant PyMuPDF installation (consider removing in production)\n# This line appears to be a duplicate and can be safely removed\n!pip install PyMuPDF\n\n# =============================================================================\n# LANGCHAIN ECOSYSTEM INSTALLATION\n# =============================================================================\n# Install LangChain framework components last to avoid version conflicts\n# LangChain is sensitive to dependency versions, so installing it last\n# helps prevent package resolution issues\n\n# LangChain Core - Essential base functionality for the framework\n# Contains fundamental abstractions and base classes\n!pip install --upgrade --quiet langchain-core\n\n# LangChain Community - Community-contributed integrations and tools\n# Provides additional connectors and utilities for various services\n!pip install --upgrade --quiet langchain-community\n\n# LangChain - Main framework for building applications with LLMs\n# Orchestration layer for chaining language models with other tools\n!pip install --upgrade --quiet langchain\n\n# =============================================================================\n# FINAL DEPENDENCY UPDATES AND FIXES\n# =============================================================================\n# Address specific version conflicts and ensure latest compatible versions\n\n# Update Transformers to latest version after LangChain installation\n# Ensures compatibility with the most recent model architectures\n!pip install --upgrade transformers\n\n# Force reinstall FSSpec with specific version to resolve compatibility issues\n# FSSpec handles file system operations and is critical for model loading\n# Version 2025.3.0 addresses known issues with remote file access\n!pip install fsspec==2025.3.0 --force-reinstall --quiet\n\n# =============================================================================\n# INSTALLATION COMPLETION\n# =============================================================================\n# Confirmation message for successful installation\nprint(\"Installation completed!\")\nprint(\"Environment ready for:\")\nprint(\"- Document processing (PDF, images, OCR)\")\nprint(\"- Machine learning inference (Transformers, embeddings)\")\nprint(\"- Vector similarity search (FAISS)\")\nprint(\"- Conversational AI applications (LangChain)\")\nprint(\"- Interactive web interfaces (Gradio)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clean environment setup for Kaggle\n# Step-by-step installation to avoid conflicts\nimport os\nimport sys\n\n# Set environment variables\nos.environ[\"TRANSFORMERS_CACHE\"] = \"/kaggle/working/transformers_cache\"\nos.environ[\"HF_HOME\"] = \"/kaggle/working/huggingface_cache\"\n\n# Clear any problematic cached imports\nmodules_to_clear = ['torchvision', 'torch', 'transformers', 'sentence_transformers']\nfor module in modules_to_clear:\n    if module in sys.modules:\n        del sys.modules[module]\n\n# Set environment variables to avoid conflicts\nos.environ['TORCH_HOME'] = '/tmp/torch_cache'\nos.environ['TRANSFORMERS_CACHE'] = '/tmp/transformers_cache'\n\nprint(\"Environment cleaned successfully\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CUDA Warning Suppression - Run this FIRST\nimport os\nimport sys\nimport warnings\nimport logging\n\n# Comprehensive warning suppression\nwarnings.filterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\nos.environ['PYTHONWARNINGS'] = 'ignore'\n\n# Suppress specific CUDA warnings\nos.environ['TORCH_CUDA_ARCH_LIST'] = ''\nos.environ['CUDA_MODULE_LOADING'] = 'LAZY'\n\n# Set logging levels\nlogging.getLogger().setLevel(logging.ERROR)\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\nlogging.getLogger(\"torch\").setLevel(logging.ERROR)\n\nprint(\"Warning suppression activated\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Alternative approach without sentence-transformers\nimport warnings\nwarnings.filterwarnings('ignore')\nimport torch\nimport transformers\nfrom transformers import PreTrainedModel, pipeline, AutoTokenizer, AutoModelForCausalLM, AutoModel\nimport torch.nn.functional as F\n\nprint(\"Testing Kaggle environment...\")\nprint(f\"Transformers version: {transformers.__version__}\")\n\ntry:\n    # Use transformers directly for embeddings instead of sentence-transformers\n    embedding_model_name = \"sentence-transformers/paraphrase-MiniLM-L3-v2\"\n    embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n    embedding_model = AutoModel.from_pretrained(embedding_model_name)\n    print(\"Embedding model loaded successfully (via transformers)\")\n    \n    # Load LLM\n    tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    llm_pipeline = pipeline(\n        \"text-generation\",\n        model=\"distilgpt2\",\n        tokenizer=tokenizer,\n        max_new_tokens=50,\n        device=0 if torch.cuda.is_available() else -1,\n        truncation=True\n    )\n    print(\"Language model loaded successfully\")\n    \n    MODELS_LOADED = True\n    print(\"All models ready!\")\n    \nexcept Exception as e:\n    print(f\"Model loading error: {e}\")\n    MODELS_LOADED = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# üñºÔ∏è Kaggle-Compatible Image Processing\nfrom PIL import Image\nfrom pathlib import Path\nimport tempfile\n\n# Simplified image captioning for Kaggle environment\ndef generate_caption(image_path: str) -> str:\n    \"\"\"Generate simple caption for images - Kaggle optimized.\"\"\"\n    try:\n        # Just return filename-based description (no heavy models)\n        filename = Path(image_path).name\n        return f\"Image file: {filename}\"\n    except Exception as e:\n        return f\"Image processing error: {str(e)}\"\n\nprint(\"‚úÖ Image processing setup completed\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install PyMuPDF\n\n# üìÑ Kaggle-Compatible PDF Processing\nimport fitz  # PyMuPDF\nimport tempfile\nfrom pathlib import Path\nfrom typing import List\n\ndef extract_chunks_from_pdf(pdf_bytes: bytes) -> List[str]:\n    \"\"\"Extract chunks from PDF - Kaggle optimized.\"\"\"\n    print(\"üîÑ Processing PDF...\")\n    \n    try:\n        doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n        all_chunks = []\n        \n        # Simple text splitter for Kaggle\n        def simple_split_text(text, chunk_size=200):\n            words = text.split()\n            chunks = []\n            current_chunk = []\n            current_length = 0\n            \n            for word in words:\n                if current_length + len(word) > chunk_size and current_chunk:\n                    chunks.append(\" \".join(current_chunk))\n                    current_chunk = [word]\n                    current_length = len(word)\n                else:\n                    current_chunk.append(word)\n                    current_length += len(word) + 1\n            \n            if current_chunk:\n                chunks.append(\" \".join(current_chunk))\n            return chunks\n\n        for page_num in range(min(len(doc), 10)):  # Limit to 10 pages for Kaggle\n            page = doc[page_num]\n            \n            # Extract text\n            text = page.get_text(\"text\").strip()\n            if text:\n                text_chunks = simple_split_text(text)\n                for chunk in text_chunks:\n                    if len(chunk.strip()) > 20:  # Filter very short chunks\n                        all_chunks.append(f\"Page {page_num+1}: {chunk.strip()}\")\n            \n            # Simple image handling (no heavy processing)\n            try:\n                images = page.get_images(full=True)\n                if images:\n                    all_chunks.append(f\"Page {page_num+1}: Contains {len(images)} image(s)\")\n            except:\n                pass  # Skip image errors\n        \n        doc.close()\n        print(f\"‚úÖ Extracted {len(all_chunks)} chunks from PDF\")\n        return all_chunks[:50]  # Limit chunks for Kaggle performance\n        \n    except Exception as e:\n        print(f\"‚ùå PDF processing error: {e}\")\n        return [f\"Error processing PDF: {str(e)}\"]\n\nprint(\"‚úÖ PDF processing setup completed\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Install new package\nimport subprocess\nimport sys\n\nprint(\"Installing updated LangChain HuggingFace package...\")\nsubprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"langchain-huggingface\"])\n\n# Step 2: Test the new import\ntry:\n    from langchain_huggingface import HuggingFaceEmbeddings\n    print(\"‚úì New HuggingFaceEmbeddings import successful!\")\n    \n    # Test creating embeddings\n    embeddings = HuggingFaceEmbeddings(\n        model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\"\n    )\n    print(\"‚úì HuggingFaceEmbeddings object created successfully\")\n    \nexcept ImportError as e:\n    print(f\"Import failed: {e}\")\n    print(\"Falling back to old import...\")\n    try:\n        from langchain.embeddings import HuggingFaceEmbeddings\n        print(\"‚úì Using deprecated version (still works)\")\n    except ImportError:\n        print(\"‚úó Both imports failed\")\n\n!pip install faiss-cpu\n!pip install --upgrade --quiet langchain-community","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# üîó Kaggle-Optimized LangChain Pipeline\nfrom langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\nfrom transformers import pipeline\nfrom typing import List\nimport torch\n\ndef build_langchain_pipeline(chunks: List[str]):\n    \"\"\"Build Kaggle-optimized pipeline.\"\"\"\n    \n    print(f\"üîÑ Building pipeline with {len(chunks)} chunks...\")\n    \n    try:\n        # 1. Kaggle-optimized embeddings\n        print(\"üì¶ Setting up embeddings...\")\n        embeddings = HuggingFaceEmbeddings(\n            model_name=\"paraphrase-MiniLM-L3-v2\",  # Small, fast model\n            model_kwargs={\n                'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n                'trust_remote_code': True\n            },\n            encode_kwargs={'normalize_embeddings': True, 'batch_size': 32}\n        )\n        \n        # 2. Create vectorstore with error handling\n        print(\"üèóÔ∏è Creating vector database...\")\n        try:\n            # Process in smaller batches for Kaggle\n            if len(chunks) > 20:\n                chunks = chunks[:20]  # Limit for Kaggle performance\n            \n            vectorstore = FAISS.from_texts(chunks, embeddings)\n            print(f\"‚úÖ Vector database created with {len(chunks)} chunks\")\n            \n        except Exception as e:\n            print(f\"‚ö†Ô∏è Vectorstore error: {e}\")\n            # Fallback with minimal content\n            safe_chunks = [\"Document loaded and ready for questions.\"]\n            vectorstore = FAISS.from_texts(safe_chunks, embeddings)\n        \n        retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})  # Reduced for performance\n        \n        # 3. Kaggle-optimized LLM\n        print(\"ü§ñ Setting up language model...\")\n        \n        try:\n            generator = pipeline(\n                'text-generation',\n                model='Qwen/Qwen2-0.5B-Instruct',\n                tokenizer='Qwen/Qwen2-0.5B-Instruct',\n                max_new_tokens=200,  # Increased for better detail\n                do_sample=True,\n                temperature=0.1,  # Lower for coherent, meaningful responses\n                top_p=0.95,\n                pad_token_id=50256,\n                device=0 if torch.cuda.is_available() else -1,\n                truncation=True\n            )\n            \n            llm = HuggingFacePipeline(pipeline=generator)\n            print(\"‚úÖ Language model loaded\")\n            \n        except Exception as e:\n            print(f\"‚ö†Ô∏è LLM error: {e}\")\n            # Create simple fallback\n            class SimpleLLM:\n                def __call__(self, prompt, **kwargs):\n                    return \"I can help answer questions about your document. Please ask specific questions.\"\n                \n                def invoke(self, input_dict, **kwargs):\n                    return self(\"\", **kwargs)\n                    \n                def _call(self, prompt, **kwargs):\n                    return self(prompt, **kwargs)\n            \n            llm = SimpleLLM()\n        \n        # 4. Simple prompt for Kaggle\n        prompt_template = \"\"\"Use the context to answer the question. Be concise. If you dont know the answer just say sorry this is not possible\n\nContext: {context}\nQuestion: {question}\n\nAnswer:\"\"\"\n        \n        prompt = PromptTemplate(\n            template=prompt_template,\n            input_variables=[\"context\", \"question\"]\n        )\n        \n        # 5. Create simple retrieval chain\n        chain = RetrievalQA.from_chain_type(\n            llm=llm,\n            chain_type=\"stuff\",\n            retriever=retriever,\n            chain_type_kwargs={\"prompt\": prompt},\n            return_source_documents=False\n        )\n        \n        print(\"üöÄ Pipeline created successfully!\")\n        return chain\n        \n    except Exception as e:\n        print(f\"‚ùå Pipeline creation error: {e}\")\n        # Return minimal working chain\n        class MinimalChain:\n            def __call__(self, inputs):\n                return {\"result\": \"Document loaded. Please ask questions about the content.\"}\n            \n            def invoke(self, inputs):\n                return self(inputs)\n        \n        return MinimalChain()\n\nprint(\"‚úÖ Pipeline setup completed\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# üé® Kaggle-Compatible Gradio Interface\nimport gradio as gr\n\n# Session storage\n_session = {\n    \"chain\": None, \n    \"status\": \"Ready\",\n    \"chat_history\": []  # Add this line\n}\n\ndef ingest_pdf(file):\n    \"\"\"Process uploaded PDF - Kaggle optimized.\"\"\"\n    if not file:\n        return \"‚ùå No file uploaded\"\n    \n    try:\n        _session[\"status\"] = \"Processing...\"\n        \n        # Read PDF\n        print(\"üìñ Reading PDF file...\")\n        with open(file.name, \"rb\") as f:\n            pdf_bytes = f.read()\n        \n        # Extract chunks\n        print(\"üîÑ Extracting content...\")\n        chunks = extract_chunks_from_pdf(pdf_bytes)\n        \n        if not chunks:\n            return \"‚ùå No content extracted from PDF\"\n        \n        # Build pipeline\n        print(\"üîó Building Q&A pipeline...\")\n        chain = build_langchain_pipeline(chunks)\n        _session[\"chain\"] = chain\n        _session[\"status\"] = \"Ready\"\n        _session[\"chat_history\"] = []  # Add this line to clear history on new PDF\n        \n        return f\"‚úÖ PDF processed! Extracted {len(chunks)} chunks. Ready for questions.\"\n        \n    except Exception as e:\n        _session[\"status\"] = \"Error\"\n        error_msg = f\"‚ùå Error: {str(e)}\"\n        print(error_msg)\n        return error_msg\n\ndef ask_question(question: str, chat_history):\n    if not question.strip():\n        return chat_history, chat_history, \"\"\n    \n    if _session.get(\"chain\") is None:\n        error_msg = \"‚ùå Please upload and process a PDF first.\"\n        chat_history.append([\"‚ùå Error\", error_msg])\n        return chat_history, chat_history, \"\"\n    \n    try:\n        print(f\"ü§î Processing question: {question}\")\n        \n        # Build context from chat history for better continuity\n        context_messages = []\n        if _session.get(\"chat_history\"):\n            # Include last 3 Q&A pairs for context (to avoid token limits)\n            recent_history = _session[\"chat_history\"][-3:]\n            for hist_q, hist_a in recent_history:\n                context_messages.append(f\"Previous Q: {hist_q}\")\n                context_messages.append(f\"Previous A: {hist_a}\")\n        \n        # Create enhanced query with context\n        if context_messages:\n            context_str = \"\\n\".join(context_messages)\n            enhanced_query = f\"Previous conversation context:\\n{context_str}\\n\\nCurrent question: {question}\"\n        else:\n            enhanced_query = question\n        \n        chain = _session[\"chain\"]\n        \n        if hasattr(chain, 'invoke'):\n            response = chain.invoke({\"query\": enhanced_query})\n        elif hasattr(chain, '__call__'):\n            response = chain({\"query\": enhanced_query})\n        else:\n            response = {\"result\": \"Unable to process question with current setup.\"}\n        \n        if isinstance(response, dict):\n            raw_answer = response.get(\"result\", response.get(\"answer\", str(response)))\n        else:\n            raw_answer = str(response)\n        \n        # Extract only after \"Answer:\"\n        answer = raw_answer.split(\"Answer:\")[-1].strip() if \"Answer:\" in raw_answer else raw_answer.strip()\n        \n        # Store in session history\n        _session[\"chat_history\"].append((question, answer))\n        \n        # Update chat interface\n        chat_history.append([question, answer])\n        \n        print(\"‚úÖ Answer generated\")\n        return chat_history, chat_history, \"\"  # Return updated history and clear input\n        \n    except Exception as e:\n        error_msg = f\"‚ùå Error answering question: {str(e)}\"\n        print(error_msg)\n        chat_history.append([question, error_msg])\n        return chat_history, chat_history, \"\"\n\ndef clear_chat():\n    \"\"\"Clear chat history\"\"\"\n    _session[\"chat_history\"] = []\n    return [], []\n\ndef reset_session():\n    \"\"\"Reset entire session\"\"\"\n    _session[\"chain\"] = None\n    _session[\"status\"] = \"Ready\"\n    _session[\"chat_history\"] = []\n    return [], [], \"Session reset. Please upload a new PDF.\"\n        \n# Create Kaggle-optimized Gradio interface\ndef create_interface():\n    with gr.Blocks(title=\"PDF Q&A - Kaggle Edition\") as demo:\n        gr.Markdown(\"# üìö PDF Question & Answer System\")\n        gr.Markdown(\"*Powered by Open Source Models - Optimized for Kaggle*\")\n        \n        with gr.Row():\n            with gr.Column():\n                file_upload = gr.File(\n                    label=\"üìé Upload PDF\", \n                    file_types=[\".pdf\"],\n                    file_count=\"single\"\n                )\n                process_btn = gr.Button(\"üöÄ Process PDF\", variant=\"primary\")\n                \n            with gr.Column():\n                status_output = gr.Textbox(\n                    label=\"üìä Status\", \n                    value=\"Ready to process PDF...\",\n                    interactive=False\n                )\n        \n        gr.Markdown(\"---\")\n        \n        # Chat Interface\n        with gr.Row():\n            with gr.Column(scale=4):\n                chatbot = gr.Chatbot(\n                    label=\"üí¨ Chat History\",\n                    height=400,\n                    show_label=True\n                )\n                \n            with gr.Column(scale=1):\n                clear_btn = gr.Button(\"üóëÔ∏è Clear Chat\", variant=\"secondary\")\n                reset_btn = gr.Button(\"üîÑ Reset Session\", variant=\"stop\")\n        \n        with gr.Row():\n            with gr.Column(scale=4):\n                question_input = gr.Textbox(\n                    label=\"‚ùì Your Question\",\n                    placeholder=\"Ask anything about your PDF... (supports follow-up questions)\",\n                    lines=2\n                )\n            with gr.Column(scale=1):\n                ask_btn = gr.Button(\"üéØ Ask Question\", variant=\"primary\")\n        \n        # Event handlers\n        process_btn.click(\n            fn=ingest_pdf,\n            inputs=[file_upload],\n            outputs=[status_output]\n        )\n        \n        ask_btn.click(\n            fn=ask_question,\n            inputs=[question_input, chatbot],\n            outputs=[chatbot, chatbot, question_input]\n        )\n        \n        # Allow Enter key for questions\n        question_input.submit(\n            fn=ask_question,\n            inputs=[question_input, chatbot],\n            outputs=[chatbot, chatbot, question_input]\n        )\n        \n        # Clear and reset handlers\n        clear_btn.click(\n            fn=clear_chat,\n            outputs=[chatbot, chatbot]\n        )\n        \n        reset_btn.click(\n            fn=reset_session,\n            outputs=[chatbot, chatbot, status_output]\n        )\n    \n    return demo\n# Launch interface\nprint(\"üöÄ Starting Gradio interface...\")\ndemo = create_interface()\ndemo.launch()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}